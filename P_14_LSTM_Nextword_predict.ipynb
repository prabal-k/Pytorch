{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 414,
      "metadata": {
        "id": "coUiUcbiMF-1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_aOesJVMlJg",
        "outputId": "ba7d0828-cbc8-42e7-fec0-36d1af3144d7"
      },
      "execution_count": 415,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = \"\"\"About the Program\n",
        "What is the course fee for  Data Science Mentorship Program (DSMP 2023)\n",
        "The course follows a monthly subscription model where you have to make monthly payments of Rs 799/month.\n",
        "What is the total duration of the course?\n",
        "The total duration of the course is 7 months. So the total course fee becomes 799*7 = Rs 5600(approx.)\n",
        "What is the syllabus of the mentorship program?\n",
        "We will be covering the following modules:\n",
        "Python Fundamentals\n",
        "Python libraries for Data Science\n",
        "Data Analysis\n",
        "SQL for Data Science\n",
        "Maths for Machine Learning\n",
        "ML Algorithms\n",
        "Practical ML\n",
        "MLOPs\n",
        "Case studies\n",
        "You can check the detailed syllabus here - https://learnwith.campusx.in/courses/CampusX-Data-Science-Mentorship-Program-637339afe4b0615a1bbed390\n",
        "Will Deep Learning and NLP be a part of this program?\n",
        "No, NLP and Deep Learning both are not a part of this program’s curriculum.\n",
        "What if I miss a live session? Will I get a recording of the session?\n",
        "Yes all our sessions are recorded, so even if you miss a session you can go back and watch the recording.\n",
        "Where can I find the class schedule?\n",
        "Checkout this google sheet to see month by month time table of the course - https://docs.google.com/spreadsheets/d/16OoTax_A6ORAeCg4emgexhqqPv3noQPYKU7RJ6ArOzk/edit?usp=sharing.\n",
        "What is the time duration of all the live sessions?\n",
        "Roughly, all the sessions last 2 hours.\n",
        "What is the language spoken by the instructor during the sessions?\n",
        "Hinglish\n",
        "How will I be informed about the upcoming class?\n",
        "You will get a mail from our side before every paid session once you become a paid user.\n",
        "Can I do this course if I am from a non-tech background?\n",
        "Yes, absolutely.\n",
        "I am late, can I join the program in the middle?\n",
        "Absolutely, you can join the program anytime.\n",
        "If I join/pay in the middle, will I be able to see all the past lectures?\n",
        "Yes, once you make the payment you will be able to see all the past content in your dashboard.\n",
        "Where do I have to submit the task?\n",
        "You don’t have to submit the task. We will provide you with the solutions, you have to self evaluate the task yourself.\n",
        "Will we do case studies in the program?\n",
        "Yes.\n",
        "Where can we contact you?\n",
        "You can mail us at nitish.campusx@gmail.com\n",
        "Payment/Registration related questions\n",
        "Where do we have to make our payments? Your YouTube channel or website?\n",
        "You have to make all your monthly payments on our website. Here is the link for our website - https://learnwith.campusx.in/\n",
        "Can we pay the entire amount of Rs 5600 all at once?\n",
        "Unfortunately no, the program follows a monthly subscription model.\n",
        "What is the validity of monthly subscription? Suppose if I pay on 15th Jan, then do I have to pay again on 1st Feb or 15th Feb\n",
        "15th Feb. The validity period is 30 days from the day you make the payment. So essentially you can join anytime you don’t have to wait for a month to end.\n",
        "What if I don’t like the course after making the payment. What is the refund policy?\n",
        "You get a 7 days refund period from the day you have made the payment.\n",
        "I am living outside India and I am not able to make the payment on the website, what should I do?\n",
        "You have to contact us by sending a mail at nitish.campusx@gmail.com\n",
        "Post registration queries\n",
        "Till when can I view the paid videos on the website?\n",
        "This one is tricky, so read carefully. You can watch the videos till your subscription is valid. Suppose you have purchased subscription on 21st Jan, you will be able to watch all the past paid sessions in the period of 21st Jan to 20th Feb. But after 21st Feb you will have to purchase the subscription again.\n",
        "But once the course is over and you have paid us Rs 5600(or 7 installments of Rs 799) you will be able to watch the paid sessions till Aug 2024.\n",
        "Why lifetime validity is not provided?\n",
        "Because of the low course fee.\n",
        "Where can I reach out in case of a doubt after the session?\n",
        "You will have to fill a google form provided in your dashboard and our team will contact you for a 1 on 1 doubt clearance session\n",
        "If I join the program late, can I still ask past week doubts?\n",
        "Yes, just select past week doubt in the doubt clearance google form.\n",
        "I am living outside India and I am not able to make the payment on the website, what should I do?\n",
        "You have to contact us by sending a mail at nitish.campusx@gmai.com\n",
        "Certificate and Placement Assistance related queries\n",
        "What is the criteria to get the certificate?\n",
        "There are 2 criterias:\n",
        "You have to pay the entire fee of Rs 5600\n",
        "You have to attempt all the course assessments.\n",
        "I am joining late. How can I pay payment of the earlier months?\n",
        "You will get a link to pay fee of earlier months in your dashboard once you pay for the current month.\n",
        "I have read that Placement assistance is a part of this program. What comes under Placement assistance?\n",
        "This is to clarify that Placement assistance does not mean Placement guarantee. So we dont guarantee you any jobs or for that matter even interview calls. So if you are planning to join this course just for placements, I am afraid you will be disappointed. Here is what comes under placement assistance\n",
        "Portfolio Building sessions\n",
        "Soft skill sessions\n",
        "Sessions with industry mentors\n",
        "Discussion on Job hunting strategies\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "cHb5rdSIM6dZ"
      },
      "execution_count": 416,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Data Preprocessing (tokenization and lowercase)"
      ],
      "metadata": {
        "id": "jI0j7vvbNWrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR75SlmmNJV8",
        "outputId": "d422458f-f9f2-4f7e-b863-d292655a4ceb"
      },
      "execution_count": 417,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 417
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-1 Data Preprocessing (Tokenization and lower case)"
      ],
      "metadata": {
        "id": "ffA1DJdqjGG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = document.split('\\n')"
      ],
      "metadata": {
        "id": "B9oFrqsojhU0"
      },
      "execution_count": 418,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPb-k7EGjmCK",
        "outputId": "30a05987-acad-4f66-cc12-782f085ef28e"
      },
      "execution_count": 419,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['About the Program',\n",
              " 'What is the course fee for  Data Science Mentorship Program (DSMP 2023)',\n",
              " 'The course follows a monthly subscription model where you have to make monthly payments of Rs 799/month.',\n",
              " 'What is the total duration of the course?',\n",
              " 'The total duration of the course is 7 months. So the total course fee becomes 799*7 = Rs 5600(approx.)',\n",
              " 'What is the syllabus of the mentorship program?',\n",
              " 'We will be covering the following modules:',\n",
              " 'Python Fundamentals',\n",
              " 'Python libraries for Data Science',\n",
              " 'Data Analysis',\n",
              " 'SQL for Data Science',\n",
              " 'Maths for Machine Learning',\n",
              " 'ML Algorithms',\n",
              " 'Practical ML',\n",
              " 'MLOPs',\n",
              " 'Case studies',\n",
              " 'You can check the detailed syllabus here - https://learnwith.campusx.in/courses/CampusX-Data-Science-Mentorship-Program-637339afe4b0615a1bbed390',\n",
              " 'Will Deep Learning and NLP be a part of this program?',\n",
              " 'No, NLP and Deep Learning both are not a part of this program’s curriculum.',\n",
              " 'What if I miss a live session? Will I get a recording of the session?',\n",
              " 'Yes all our sessions are recorded, so even if you miss a session you can go back and watch the recording.',\n",
              " 'Where can I find the class schedule?',\n",
              " 'Checkout this google sheet to see month by month time table of the course - https://docs.google.com/spreadsheets/d/16OoTax_A6ORAeCg4emgexhqqPv3noQPYKU7RJ6ArOzk/edit?usp=sharing.',\n",
              " 'What is the time duration of all the live sessions?',\n",
              " 'Roughly, all the sessions last 2 hours.',\n",
              " 'What is the language spoken by the instructor during the sessions?',\n",
              " 'Hinglish',\n",
              " 'How will I be informed about the upcoming class?',\n",
              " 'You will get a mail from our side before every paid session once you become a paid user.',\n",
              " 'Can I do this course if I am from a non-tech background?',\n",
              " 'Yes, absolutely.',\n",
              " 'I am late, can I join the program in the middle?',\n",
              " 'Absolutely, you can join the program anytime.',\n",
              " 'If I join/pay in the middle, will I be able to see all the past lectures?',\n",
              " 'Yes, once you make the payment you will be able to see all the past content in your dashboard.',\n",
              " 'Where do I have to submit the task?',\n",
              " 'You don’t have to submit the task. We will provide you with the solutions, you have to self evaluate the task yourself.',\n",
              " 'Will we do case studies in the program?',\n",
              " 'Yes.',\n",
              " 'Where can we contact you?',\n",
              " 'You can mail us at nitish.campusx@gmail.com',\n",
              " 'Payment/Registration related questions',\n",
              " 'Where do we have to make our payments? Your YouTube channel or website?',\n",
              " 'You have to make all your monthly payments on our website. Here is the link for our website - https://learnwith.campusx.in/',\n",
              " 'Can we pay the entire amount of Rs 5600 all at once?',\n",
              " 'Unfortunately no, the program follows a monthly subscription model.',\n",
              " 'What is the validity of monthly subscription? Suppose if I pay on 15th Jan, then do I have to pay again on 1st Feb or 15th Feb',\n",
              " '15th Feb. The validity period is 30 days from the day you make the payment. So essentially you can join anytime you don’t have to wait for a month to end.',\n",
              " 'What if I don’t like the course after making the payment. What is the refund policy?',\n",
              " 'You get a 7 days refund period from the day you have made the payment.',\n",
              " 'I am living outside India and I am not able to make the payment on the website, what should I do?',\n",
              " 'You have to contact us by sending a mail at nitish.campusx@gmail.com',\n",
              " 'Post registration queries',\n",
              " 'Till when can I view the paid videos on the website?',\n",
              " 'This one is tricky, so read carefully. You can watch the videos till your subscription is valid. Suppose you have purchased subscription on 21st Jan, you will be able to watch all the past paid sessions in the period of 21st Jan to 20th Feb. But after 21st Feb you will have to purchase the subscription again.',\n",
              " 'But once the course is over and you have paid us Rs 5600(or 7 installments of Rs 799) you will be able to watch the paid sessions till Aug 2024.',\n",
              " 'Why lifetime validity is not provided?',\n",
              " 'Because of the low course fee.',\n",
              " 'Where can I reach out in case of a doubt after the session?',\n",
              " 'You will have to fill a google form provided in your dashboard and our team will contact you for a 1 on 1 doubt clearance session',\n",
              " 'If I join the program late, can I still ask past week doubts?',\n",
              " 'Yes, just select past week doubt in the doubt clearance google form.',\n",
              " 'I am living outside India and I am not able to make the payment on the website, what should I do?',\n",
              " 'You have to contact us by sending a mail at nitish.campusx@gmai.com',\n",
              " 'Certificate and Placement Assistance related queries',\n",
              " 'What is the criteria to get the certificate?',\n",
              " 'There are 2 criterias:',\n",
              " 'You have to pay the entire fee of Rs 5600',\n",
              " 'You have to attempt all the course assessments.',\n",
              " 'I am joining late. How can I pay payment of the earlier months?',\n",
              " 'You will get a link to pay fee of earlier months in your dashboard once you pay for the current month.',\n",
              " 'I have read that Placement assistance is a part of this program. What comes under Placement assistance?',\n",
              " 'This is to clarify that Placement assistance does not mean Placement guarantee. So we dont guarantee you any jobs or for that matter even interview calls. So if you are planning to join this course just for placements, I am afraid you will be disappointed. Here is what comes under placement assistance',\n",
              " 'Portfolio Building sessions',\n",
              " 'Soft skill sessions',\n",
              " 'Sessions with industry mentors',\n",
              " 'Discussion on Job hunting strategies',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 419
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_doc = []\n",
        "for sentence in document:\n",
        "  # print(sentence)\n",
        "  tokenized_doc.append(word_tokenize(sentence.lower()))"
      ],
      "metadata": {
        "id": "yM9cLwF-iYRC"
      },
      "execution_count": 420,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-2 Creating a Vocabulary dictionary"
      ],
      "metadata": {
        "id": "gU8-w8mLmt2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {'<unk>':0}"
      ],
      "metadata": {
        "id": "YCMFd7gfnBEu"
      },
      "execution_count": 421,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in tokenized_doc:\n",
        "  for token in sentence:\n",
        "    if token not in vocab:\n",
        "      vocab[token] = len(vocab)"
      ],
      "metadata": {
        "id": "LZcpqEHZmxOi"
      },
      "execution_count": 422,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NyRcRgonMMH",
        "outputId": "6a7cf0c9-8e6a-430f-8a7d-4a32962cb45a"
      },
      "execution_count": 423,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<unk>': 0,\n",
              " 'about': 1,\n",
              " 'the': 2,\n",
              " 'program': 3,\n",
              " 'what': 4,\n",
              " 'is': 5,\n",
              " 'course': 6,\n",
              " 'fee': 7,\n",
              " 'for': 8,\n",
              " 'data': 9,\n",
              " 'science': 10,\n",
              " 'mentorship': 11,\n",
              " '(': 12,\n",
              " 'dsmp': 13,\n",
              " '2023': 14,\n",
              " ')': 15,\n",
              " 'follows': 16,\n",
              " 'a': 17,\n",
              " 'monthly': 18,\n",
              " 'subscription': 19,\n",
              " 'model': 20,\n",
              " 'where': 21,\n",
              " 'you': 22,\n",
              " 'have': 23,\n",
              " 'to': 24,\n",
              " 'make': 25,\n",
              " 'payments': 26,\n",
              " 'of': 27,\n",
              " 'rs': 28,\n",
              " '799/month': 29,\n",
              " '.': 30,\n",
              " 'total': 31,\n",
              " 'duration': 32,\n",
              " '?': 33,\n",
              " '7': 34,\n",
              " 'months': 35,\n",
              " 'so': 36,\n",
              " 'becomes': 37,\n",
              " '799': 38,\n",
              " '*': 39,\n",
              " '=': 40,\n",
              " '5600': 41,\n",
              " 'approx': 42,\n",
              " 'syllabus': 43,\n",
              " 'we': 44,\n",
              " 'will': 45,\n",
              " 'be': 46,\n",
              " 'covering': 47,\n",
              " 'following': 48,\n",
              " 'modules': 49,\n",
              " ':': 50,\n",
              " 'python': 51,\n",
              " 'fundamentals': 52,\n",
              " 'libraries': 53,\n",
              " 'analysis': 54,\n",
              " 'sql': 55,\n",
              " 'maths': 56,\n",
              " 'machine': 57,\n",
              " 'learning': 58,\n",
              " 'ml': 59,\n",
              " 'algorithms': 60,\n",
              " 'practical': 61,\n",
              " 'mlops': 62,\n",
              " 'case': 63,\n",
              " 'studies': 64,\n",
              " 'can': 65,\n",
              " 'check': 66,\n",
              " 'detailed': 67,\n",
              " 'here': 68,\n",
              " '-': 69,\n",
              " 'https': 70,\n",
              " '//learnwith.campusx.in/courses/campusx-data-science-mentorship-program-637339afe4b0615a1bbed390': 71,\n",
              " 'deep': 72,\n",
              " 'and': 73,\n",
              " 'nlp': 74,\n",
              " 'part': 75,\n",
              " 'this': 76,\n",
              " 'no': 77,\n",
              " ',': 78,\n",
              " 'both': 79,\n",
              " 'are': 80,\n",
              " 'not': 81,\n",
              " '’': 82,\n",
              " 's': 83,\n",
              " 'curriculum': 84,\n",
              " 'if': 85,\n",
              " 'i': 86,\n",
              " 'miss': 87,\n",
              " 'live': 88,\n",
              " 'session': 89,\n",
              " 'get': 90,\n",
              " 'recording': 91,\n",
              " 'yes': 92,\n",
              " 'all': 93,\n",
              " 'our': 94,\n",
              " 'sessions': 95,\n",
              " 'recorded': 96,\n",
              " 'even': 97,\n",
              " 'go': 98,\n",
              " 'back': 99,\n",
              " 'watch': 100,\n",
              " 'find': 101,\n",
              " 'class': 102,\n",
              " 'schedule': 103,\n",
              " 'checkout': 104,\n",
              " 'google': 105,\n",
              " 'sheet': 106,\n",
              " 'see': 107,\n",
              " 'month': 108,\n",
              " 'by': 109,\n",
              " 'time': 110,\n",
              " 'table': 111,\n",
              " '//docs.google.com/spreadsheets/d/16ootax_a6oraecg4emgexhqqpv3noqpyku7rj6arozk/edit': 112,\n",
              " 'usp=sharing': 113,\n",
              " 'roughly': 114,\n",
              " 'last': 115,\n",
              " '2': 116,\n",
              " 'hours': 117,\n",
              " 'language': 118,\n",
              " 'spoken': 119,\n",
              " 'instructor': 120,\n",
              " 'during': 121,\n",
              " 'hinglish': 122,\n",
              " 'how': 123,\n",
              " 'informed': 124,\n",
              " 'upcoming': 125,\n",
              " 'mail': 126,\n",
              " 'from': 127,\n",
              " 'side': 128,\n",
              " 'before': 129,\n",
              " 'every': 130,\n",
              " 'paid': 131,\n",
              " 'once': 132,\n",
              " 'become': 133,\n",
              " 'user': 134,\n",
              " 'do': 135,\n",
              " 'am': 136,\n",
              " 'non-tech': 137,\n",
              " 'background': 138,\n",
              " 'absolutely': 139,\n",
              " 'late': 140,\n",
              " 'join': 141,\n",
              " 'in': 142,\n",
              " 'middle': 143,\n",
              " 'anytime': 144,\n",
              " 'join/pay': 145,\n",
              " 'able': 146,\n",
              " 'past': 147,\n",
              " 'lectures': 148,\n",
              " 'payment': 149,\n",
              " 'content': 150,\n",
              " 'your': 151,\n",
              " 'dashboard': 152,\n",
              " 'submit': 153,\n",
              " 'task': 154,\n",
              " 'don': 155,\n",
              " 't': 156,\n",
              " 'provide': 157,\n",
              " 'with': 158,\n",
              " 'solutions': 159,\n",
              " 'self': 160,\n",
              " 'evaluate': 161,\n",
              " 'yourself': 162,\n",
              " 'contact': 163,\n",
              " 'us': 164,\n",
              " 'at': 165,\n",
              " 'nitish.campusx': 166,\n",
              " '@': 167,\n",
              " 'gmail.com': 168,\n",
              " 'payment/registration': 169,\n",
              " 'related': 170,\n",
              " 'questions': 171,\n",
              " 'youtube': 172,\n",
              " 'channel': 173,\n",
              " 'or': 174,\n",
              " 'website': 175,\n",
              " 'on': 176,\n",
              " 'link': 177,\n",
              " '//learnwith.campusx.in/': 178,\n",
              " 'pay': 179,\n",
              " 'entire': 180,\n",
              " 'amount': 181,\n",
              " 'unfortunately': 182,\n",
              " 'validity': 183,\n",
              " 'suppose': 184,\n",
              " '15th': 185,\n",
              " 'jan': 186,\n",
              " 'then': 187,\n",
              " 'again': 188,\n",
              " '1st': 189,\n",
              " 'feb': 190,\n",
              " 'feb.': 191,\n",
              " 'period': 192,\n",
              " '30': 193,\n",
              " 'days': 194,\n",
              " 'day': 195,\n",
              " 'essentially': 196,\n",
              " 'wait': 197,\n",
              " 'end': 198,\n",
              " 'like': 199,\n",
              " 'after': 200,\n",
              " 'making': 201,\n",
              " 'refund': 202,\n",
              " 'policy': 203,\n",
              " 'made': 204,\n",
              " 'living': 205,\n",
              " 'outside': 206,\n",
              " 'india': 207,\n",
              " 'should': 208,\n",
              " 'sending': 209,\n",
              " 'post': 210,\n",
              " 'registration': 211,\n",
              " 'queries': 212,\n",
              " 'till': 213,\n",
              " 'when': 214,\n",
              " 'view': 215,\n",
              " 'videos': 216,\n",
              " 'one': 217,\n",
              " 'tricky': 218,\n",
              " 'read': 219,\n",
              " 'carefully': 220,\n",
              " 'valid': 221,\n",
              " 'purchased': 222,\n",
              " '21st': 223,\n",
              " '20th': 224,\n",
              " 'but': 225,\n",
              " 'purchase': 226,\n",
              " 'over': 227,\n",
              " 'installments': 228,\n",
              " 'aug': 229,\n",
              " '2024': 230,\n",
              " 'why': 231,\n",
              " 'lifetime': 232,\n",
              " 'provided': 233,\n",
              " 'because': 234,\n",
              " 'low': 235,\n",
              " 'reach': 236,\n",
              " 'out': 237,\n",
              " 'doubt': 238,\n",
              " 'fill': 239,\n",
              " 'form': 240,\n",
              " 'team': 241,\n",
              " '1': 242,\n",
              " 'clearance': 243,\n",
              " 'still': 244,\n",
              " 'ask': 245,\n",
              " 'week': 246,\n",
              " 'doubts': 247,\n",
              " 'just': 248,\n",
              " 'select': 249,\n",
              " 'gmai.com': 250,\n",
              " 'certificate': 251,\n",
              " 'placement': 252,\n",
              " 'assistance': 253,\n",
              " 'criteria': 254,\n",
              " 'there': 255,\n",
              " 'criterias': 256,\n",
              " 'attempt': 257,\n",
              " 'assessments': 258,\n",
              " 'joining': 259,\n",
              " 'earlier': 260,\n",
              " 'current': 261,\n",
              " 'that': 262,\n",
              " 'comes': 263,\n",
              " 'under': 264,\n",
              " 'clarify': 265,\n",
              " 'does': 266,\n",
              " 'mean': 267,\n",
              " 'guarantee': 268,\n",
              " 'dont': 269,\n",
              " 'any': 270,\n",
              " 'jobs': 271,\n",
              " 'matter': 272,\n",
              " 'interview': 273,\n",
              " 'calls': 274,\n",
              " 'planning': 275,\n",
              " 'placements': 276,\n",
              " 'afraid': 277,\n",
              " 'disappointed': 278,\n",
              " 'portfolio': 279,\n",
              " 'building': 280,\n",
              " 'soft': 281,\n",
              " 'skill': 282,\n",
              " 'industry': 283,\n",
              " 'mentors': 284,\n",
              " 'discussion': 285,\n",
              " 'job': 286,\n",
              " 'hunting': 287,\n",
              " 'strategies': 288}"
            ]
          },
          "metadata": {},
          "execution_count": 423
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step- 3 Converting tokens into indices"
      ],
      "metadata": {
        "id": "PVZJeo5xnTF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_indices(sentence,vocab):\n",
        "  numerical_sentence = []\n",
        "  for token in sentence:\n",
        "    if token in vocab:\n",
        "      numerical_sentence.append(vocab[token])\n",
        "    else:\n",
        "      numerical_sentence.append(vocab['<unk>'])\n",
        "\n",
        "  return numerical_sentence\n",
        ""
      ],
      "metadata": {
        "id": "uuTsncz-nZmm"
      },
      "execution_count": 424,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over aindividual sentence in the document and convert tokens to index\n",
        "input_numerical_sentence = []\n",
        "for sentence in tokenized_doc:\n",
        "  input_numerical_sentence.append(tokens_to_indices(sentence,vocab))\n"
      ],
      "metadata": {
        "id": "3qjQKJ7Unmf8"
      },
      "execution_count": 425,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_numerical_sentence) # Total number of sentence in the document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjIGyh2UqZ5s",
        "outputId": "95895cbf-4fa2-42b1-e723-1341c357f8fd"
      },
      "execution_count": 426,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "metadata": {},
          "execution_count": 426
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_numerical_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaJvCvs4s9L_",
        "outputId": "0addac5b-deb5-475d-9df4-9907be188ef0"
      },
      "execution_count": 427,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3],\n",
              " [4, 5, 2, 6, 7, 8, 9, 10, 11, 3, 12, 13, 14, 15],\n",
              " [2, 6, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 18, 26, 27, 28, 29, 30],\n",
              " [4, 5, 2, 31, 32, 27, 2, 6, 33],\n",
              " [2,\n",
              "  31,\n",
              "  32,\n",
              "  27,\n",
              "  2,\n",
              "  6,\n",
              "  5,\n",
              "  34,\n",
              "  35,\n",
              "  30,\n",
              "  36,\n",
              "  2,\n",
              "  31,\n",
              "  6,\n",
              "  7,\n",
              "  37,\n",
              "  38,\n",
              "  39,\n",
              "  34,\n",
              "  40,\n",
              "  28,\n",
              "  41,\n",
              "  12,\n",
              "  42,\n",
              "  30,\n",
              "  15],\n",
              " [4, 5, 2, 43, 27, 2, 11, 3, 33],\n",
              " [44, 45, 46, 47, 2, 48, 49, 50],\n",
              " [51, 52],\n",
              " [51, 53, 8, 9, 10],\n",
              " [9, 54],\n",
              " [55, 8, 9, 10],\n",
              " [56, 8, 57, 58],\n",
              " [59, 60],\n",
              " [61, 59],\n",
              " [62],\n",
              " [63, 64],\n",
              " [22, 65, 66, 2, 67, 43, 68, 69, 70, 50, 71],\n",
              " [45, 72, 58, 73, 74, 46, 17, 75, 27, 76, 3, 33],\n",
              " [77, 78, 74, 73, 72, 58, 79, 80, 81, 17, 75, 27, 76, 3, 82, 83, 84, 30],\n",
              " [4, 85, 86, 87, 17, 88, 89, 33, 45, 86, 90, 17, 91, 27, 2, 89, 33],\n",
              " [92,\n",
              "  93,\n",
              "  94,\n",
              "  95,\n",
              "  80,\n",
              "  96,\n",
              "  78,\n",
              "  36,\n",
              "  97,\n",
              "  85,\n",
              "  22,\n",
              "  87,\n",
              "  17,\n",
              "  89,\n",
              "  22,\n",
              "  65,\n",
              "  98,\n",
              "  99,\n",
              "  73,\n",
              "  100,\n",
              "  2,\n",
              "  91,\n",
              "  30],\n",
              " [21, 65, 86, 101, 2, 102, 103, 33],\n",
              " [104,\n",
              "  76,\n",
              "  105,\n",
              "  106,\n",
              "  24,\n",
              "  107,\n",
              "  108,\n",
              "  109,\n",
              "  108,\n",
              "  110,\n",
              "  111,\n",
              "  27,\n",
              "  2,\n",
              "  6,\n",
              "  69,\n",
              "  70,\n",
              "  50,\n",
              "  112,\n",
              "  33,\n",
              "  113,\n",
              "  30],\n",
              " [4, 5, 2, 110, 32, 27, 93, 2, 88, 95, 33],\n",
              " [114, 78, 93, 2, 95, 115, 116, 117, 30],\n",
              " [4, 5, 2, 118, 119, 109, 2, 120, 121, 2, 95, 33],\n",
              " [122],\n",
              " [123, 45, 86, 46, 124, 1, 2, 125, 102, 33],\n",
              " [22,\n",
              "  45,\n",
              "  90,\n",
              "  17,\n",
              "  126,\n",
              "  127,\n",
              "  94,\n",
              "  128,\n",
              "  129,\n",
              "  130,\n",
              "  131,\n",
              "  89,\n",
              "  132,\n",
              "  22,\n",
              "  133,\n",
              "  17,\n",
              "  131,\n",
              "  134,\n",
              "  30],\n",
              " [65, 86, 135, 76, 6, 85, 86, 136, 127, 17, 137, 138, 33],\n",
              " [92, 78, 139, 30],\n",
              " [86, 136, 140, 78, 65, 86, 141, 2, 3, 142, 2, 143, 33],\n",
              " [139, 78, 22, 65, 141, 2, 3, 144, 30],\n",
              " [85, 86, 145, 142, 2, 143, 78, 45, 86, 46, 146, 24, 107, 93, 2, 147, 148, 33],\n",
              " [92,\n",
              "  78,\n",
              "  132,\n",
              "  22,\n",
              "  25,\n",
              "  2,\n",
              "  149,\n",
              "  22,\n",
              "  45,\n",
              "  46,\n",
              "  146,\n",
              "  24,\n",
              "  107,\n",
              "  93,\n",
              "  2,\n",
              "  147,\n",
              "  150,\n",
              "  142,\n",
              "  151,\n",
              "  152,\n",
              "  30],\n",
              " [21, 135, 86, 23, 24, 153, 2, 154, 33],\n",
              " [22,\n",
              "  155,\n",
              "  82,\n",
              "  156,\n",
              "  23,\n",
              "  24,\n",
              "  153,\n",
              "  2,\n",
              "  154,\n",
              "  30,\n",
              "  44,\n",
              "  45,\n",
              "  157,\n",
              "  22,\n",
              "  158,\n",
              "  2,\n",
              "  159,\n",
              "  78,\n",
              "  22,\n",
              "  23,\n",
              "  24,\n",
              "  160,\n",
              "  161,\n",
              "  2,\n",
              "  154,\n",
              "  162,\n",
              "  30],\n",
              " [45, 44, 135, 63, 64, 142, 2, 3, 33],\n",
              " [92, 30],\n",
              " [21, 65, 44, 163, 22, 33],\n",
              " [22, 65, 126, 164, 165, 166, 167, 168],\n",
              " [169, 170, 171],\n",
              " [21, 135, 44, 23, 24, 25, 94, 26, 33, 151, 172, 173, 174, 175, 33],\n",
              " [22,\n",
              "  23,\n",
              "  24,\n",
              "  25,\n",
              "  93,\n",
              "  151,\n",
              "  18,\n",
              "  26,\n",
              "  176,\n",
              "  94,\n",
              "  175,\n",
              "  30,\n",
              "  68,\n",
              "  5,\n",
              "  2,\n",
              "  177,\n",
              "  8,\n",
              "  94,\n",
              "  175,\n",
              "  69,\n",
              "  70,\n",
              "  50,\n",
              "  178],\n",
              " [65, 44, 179, 2, 180, 181, 27, 28, 41, 93, 165, 132, 33],\n",
              " [182, 77, 78, 2, 3, 16, 17, 18, 19, 20, 30],\n",
              " [4,\n",
              "  5,\n",
              "  2,\n",
              "  183,\n",
              "  27,\n",
              "  18,\n",
              "  19,\n",
              "  33,\n",
              "  184,\n",
              "  85,\n",
              "  86,\n",
              "  179,\n",
              "  176,\n",
              "  185,\n",
              "  186,\n",
              "  78,\n",
              "  187,\n",
              "  135,\n",
              "  86,\n",
              "  23,\n",
              "  24,\n",
              "  179,\n",
              "  188,\n",
              "  176,\n",
              "  189,\n",
              "  190,\n",
              "  174,\n",
              "  185,\n",
              "  190],\n",
              " [185,\n",
              "  191,\n",
              "  2,\n",
              "  183,\n",
              "  192,\n",
              "  5,\n",
              "  193,\n",
              "  194,\n",
              "  127,\n",
              "  2,\n",
              "  195,\n",
              "  22,\n",
              "  25,\n",
              "  2,\n",
              "  149,\n",
              "  30,\n",
              "  36,\n",
              "  196,\n",
              "  22,\n",
              "  65,\n",
              "  141,\n",
              "  144,\n",
              "  22,\n",
              "  155,\n",
              "  82,\n",
              "  156,\n",
              "  23,\n",
              "  24,\n",
              "  197,\n",
              "  8,\n",
              "  17,\n",
              "  108,\n",
              "  24,\n",
              "  198,\n",
              "  30],\n",
              " [4,\n",
              "  85,\n",
              "  86,\n",
              "  155,\n",
              "  82,\n",
              "  156,\n",
              "  199,\n",
              "  2,\n",
              "  6,\n",
              "  200,\n",
              "  201,\n",
              "  2,\n",
              "  149,\n",
              "  30,\n",
              "  4,\n",
              "  5,\n",
              "  2,\n",
              "  202,\n",
              "  203,\n",
              "  33],\n",
              " [22, 90, 17, 34, 194, 202, 192, 127, 2, 195, 22, 23, 204, 2, 149, 30],\n",
              " [86,\n",
              "  136,\n",
              "  205,\n",
              "  206,\n",
              "  207,\n",
              "  73,\n",
              "  86,\n",
              "  136,\n",
              "  81,\n",
              "  146,\n",
              "  24,\n",
              "  25,\n",
              "  2,\n",
              "  149,\n",
              "  176,\n",
              "  2,\n",
              "  175,\n",
              "  78,\n",
              "  4,\n",
              "  208,\n",
              "  86,\n",
              "  135,\n",
              "  33],\n",
              " [22, 23, 24, 163, 164, 109, 209, 17, 126, 165, 166, 167, 168],\n",
              " [210, 211, 212],\n",
              " [213, 214, 65, 86, 215, 2, 131, 216, 176, 2, 175, 33],\n",
              " [76,\n",
              "  217,\n",
              "  5,\n",
              "  218,\n",
              "  78,\n",
              "  36,\n",
              "  219,\n",
              "  220,\n",
              "  30,\n",
              "  22,\n",
              "  65,\n",
              "  100,\n",
              "  2,\n",
              "  216,\n",
              "  213,\n",
              "  151,\n",
              "  19,\n",
              "  5,\n",
              "  221,\n",
              "  30,\n",
              "  184,\n",
              "  22,\n",
              "  23,\n",
              "  222,\n",
              "  19,\n",
              "  176,\n",
              "  223,\n",
              "  186,\n",
              "  78,\n",
              "  22,\n",
              "  45,\n",
              "  46,\n",
              "  146,\n",
              "  24,\n",
              "  100,\n",
              "  93,\n",
              "  2,\n",
              "  147,\n",
              "  131,\n",
              "  95,\n",
              "  142,\n",
              "  2,\n",
              "  192,\n",
              "  27,\n",
              "  223,\n",
              "  186,\n",
              "  24,\n",
              "  224,\n",
              "  191,\n",
              "  225,\n",
              "  200,\n",
              "  223,\n",
              "  190,\n",
              "  22,\n",
              "  45,\n",
              "  23,\n",
              "  24,\n",
              "  226,\n",
              "  2,\n",
              "  19,\n",
              "  188,\n",
              "  30],\n",
              " [225,\n",
              "  132,\n",
              "  2,\n",
              "  6,\n",
              "  5,\n",
              "  227,\n",
              "  73,\n",
              "  22,\n",
              "  23,\n",
              "  131,\n",
              "  164,\n",
              "  28,\n",
              "  41,\n",
              "  12,\n",
              "  174,\n",
              "  34,\n",
              "  228,\n",
              "  27,\n",
              "  28,\n",
              "  38,\n",
              "  15,\n",
              "  22,\n",
              "  45,\n",
              "  46,\n",
              "  146,\n",
              "  24,\n",
              "  100,\n",
              "  2,\n",
              "  131,\n",
              "  95,\n",
              "  213,\n",
              "  229,\n",
              "  230,\n",
              "  30],\n",
              " [231, 232, 183, 5, 81, 233, 33],\n",
              " [234, 27, 2, 235, 6, 7, 30],\n",
              " [21, 65, 86, 236, 237, 142, 63, 27, 17, 238, 200, 2, 89, 33],\n",
              " [22,\n",
              "  45,\n",
              "  23,\n",
              "  24,\n",
              "  239,\n",
              "  17,\n",
              "  105,\n",
              "  240,\n",
              "  233,\n",
              "  142,\n",
              "  151,\n",
              "  152,\n",
              "  73,\n",
              "  94,\n",
              "  241,\n",
              "  45,\n",
              "  163,\n",
              "  22,\n",
              "  8,\n",
              "  17,\n",
              "  242,\n",
              "  176,\n",
              "  242,\n",
              "  238,\n",
              "  243,\n",
              "  89],\n",
              " [85, 86, 141, 2, 3, 140, 78, 65, 86, 244, 245, 147, 246, 247, 33],\n",
              " [92, 78, 248, 249, 147, 246, 238, 142, 2, 238, 243, 105, 240, 30],\n",
              " [86,\n",
              "  136,\n",
              "  205,\n",
              "  206,\n",
              "  207,\n",
              "  73,\n",
              "  86,\n",
              "  136,\n",
              "  81,\n",
              "  146,\n",
              "  24,\n",
              "  25,\n",
              "  2,\n",
              "  149,\n",
              "  176,\n",
              "  2,\n",
              "  175,\n",
              "  78,\n",
              "  4,\n",
              "  208,\n",
              "  86,\n",
              "  135,\n",
              "  33],\n",
              " [22, 23, 24, 163, 164, 109, 209, 17, 126, 165, 166, 167, 250],\n",
              " [251, 73, 252, 253, 170, 212],\n",
              " [4, 5, 2, 254, 24, 90, 2, 251, 33],\n",
              " [255, 80, 116, 256, 50],\n",
              " [22, 23, 24, 179, 2, 180, 7, 27, 28, 41],\n",
              " [22, 23, 24, 257, 93, 2, 6, 258, 30],\n",
              " [86, 136, 259, 140, 30, 123, 65, 86, 179, 149, 27, 2, 260, 35, 33],\n",
              " [22,\n",
              "  45,\n",
              "  90,\n",
              "  17,\n",
              "  177,\n",
              "  24,\n",
              "  179,\n",
              "  7,\n",
              "  27,\n",
              "  260,\n",
              "  35,\n",
              "  142,\n",
              "  151,\n",
              "  152,\n",
              "  132,\n",
              "  22,\n",
              "  179,\n",
              "  8,\n",
              "  2,\n",
              "  261,\n",
              "  108,\n",
              "  30],\n",
              " [86,\n",
              "  23,\n",
              "  219,\n",
              "  262,\n",
              "  252,\n",
              "  253,\n",
              "  5,\n",
              "  17,\n",
              "  75,\n",
              "  27,\n",
              "  76,\n",
              "  3,\n",
              "  30,\n",
              "  4,\n",
              "  263,\n",
              "  264,\n",
              "  252,\n",
              "  253,\n",
              "  33],\n",
              " [76,\n",
              "  5,\n",
              "  24,\n",
              "  265,\n",
              "  262,\n",
              "  252,\n",
              "  253,\n",
              "  266,\n",
              "  81,\n",
              "  267,\n",
              "  252,\n",
              "  268,\n",
              "  30,\n",
              "  36,\n",
              "  44,\n",
              "  269,\n",
              "  268,\n",
              "  22,\n",
              "  270,\n",
              "  271,\n",
              "  174,\n",
              "  8,\n",
              "  262,\n",
              "  272,\n",
              "  97,\n",
              "  273,\n",
              "  274,\n",
              "  30,\n",
              "  36,\n",
              "  85,\n",
              "  22,\n",
              "  80,\n",
              "  275,\n",
              "  24,\n",
              "  141,\n",
              "  76,\n",
              "  6,\n",
              "  248,\n",
              "  8,\n",
              "  276,\n",
              "  78,\n",
              "  86,\n",
              "  136,\n",
              "  277,\n",
              "  22,\n",
              "  45,\n",
              "  46,\n",
              "  278,\n",
              "  30,\n",
              "  68,\n",
              "  5,\n",
              "  4,\n",
              "  263,\n",
              "  264,\n",
              "  252,\n",
              "  253],\n",
              " [279, 280, 95],\n",
              " [281, 282, 95],\n",
              " [95, 158, 283, 284],\n",
              " [285, 176, 286, 287, 288],\n",
              " []]"
            ]
          },
          "metadata": {},
          "execution_count": 427
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-4 Creating a Training Sequence\n",
        "\n",
        "For eg : If sentence (list of numbers ) is:\n",
        "\n",
        "[1,2,3] then from this we will create 2 new training sequence i.e\n",
        "\n",
        "a. [1,2] : from this input will be 1 and output/label(model will predict) 2\n",
        "\n",
        "b. [1,2,3] : from this input will be 1 and 2, output/label(model will predict) 3\n",
        "\n",
        "Just like predicting the next word based on previous word"
      ],
      "metadata": {
        "id": "qNBUsjT4tCTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequence = []\n",
        "for sentence in input_numerical_sentence:  # Iterate over each sentence (list of numbers)\n",
        "  for i in range(1,len(sentence)): # Iterate over indices from 1 to len(sentence)-1\n",
        "    training_sequence.append(sentence[0:i+1])  # appedning sublist from index 0 to i (inclusive)"
      ],
      "metadata": {
        "id": "O8cG35RhtFlq"
      },
      "execution_count": 428,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_sequence) # Model will be trained on 942 rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bNqIIq7gaN-Z",
        "outputId": "e505313c-ba50-4a3e-ab5a-161cdc1738d7"
      },
      "execution_count": 429,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "942"
            ]
          },
          "metadata": {},
          "execution_count": 429
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-5 Padding the input sequence(making the total no of words in each row same by adding 0) since we are now training the model in batches(batch_size = 32)\n",
        "\n",
        "-> How to perform padding:\n",
        "\n",
        "1. Find the sentence with maximum number of words\n",
        "\n",
        "2. Make all the other sentence have this max no of words in it by appending 0 (in the beginning)"
      ],
      "metadata": {
        "id": "cc3am-gga8vH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-1 Find the sentence with maximum number of words\n",
        "find_max_no_of_words = []\n",
        "for sequence in training_sequence:\n",
        "  find_max_no_of_words.append(len(sequence))\n",
        "\n",
        "print(max(find_max_no_of_words)) #The sentence with max no of words is 62"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP3fpYLuc_LH",
        "outputId": "720f64b1-7b8f-4f2a-913f-9d7018a22a3b"
      },
      "execution_count": 430,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step- 2 Padding (adding 0) to make the length/size of each training sequence equal (i.e 64)\n",
        "def padding(sequence):\n",
        "      return ([0] * (max(find_max_no_of_words)-len(sequence)) + sequence)\n",
        ""
      ],
      "metadata": {
        "id": "G0ftgBIFddW9"
      },
      "execution_count": 431,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence = []"
      ],
      "metadata": {
        "id": "mnvT1Nxqkhlw"
      },
      "execution_count": 432,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sequence in training_sequence:\n",
        "  padded_training_sequence.append(padding(sequence))\n",
        "\n",
        "# Now each sentence/Row have equal no of words"
      ],
      "metadata": {
        "id": "NvZ7JinpeOIz"
      },
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_training_sequence = torch.tensor(padded_training_sequence,dtype=torch.long)"
      ],
      "metadata": {
        "id": "Tj9NoqPnrW3a"
      },
      "execution_count": 434,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-6 Seperating the last word of each row as 'Output' label"
      ],
      "metadata": {
        "id": "UfathpVurBTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = padded_training_sequence[:, :-1]  # Removes the last element from each sequence\n",
        "y = padded_training_sequence[:, -1]   # Extracts the last element from each sequence\n"
      ],
      "metadata": {
        "id": "7LrzQtW0rPHF"
      },
      "execution_count": 435,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-7 Creating custom dataset class"
      ],
      "metadata": {
        "id": "M_MRC2DrsppK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class customdataset(Dataset):\n",
        "\n",
        "  def __init__(self,x,y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.x[index] , self.y[index]"
      ],
      "metadata": {
        "id": "21j4TQE9ssvG"
      },
      "execution_count": 436,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating object for customdataset class\n",
        "train_dataset = customdataset(x,y)"
      ],
      "metadata": {
        "id": "xjWnzA9ltJgZ"
      },
      "execution_count": 437,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[16]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39uHedvRtTHh",
        "outputId": "695b4dd6-5c81-42cd-93e2-c93be657d737"
      },
      "execution_count": 438,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6]),\n",
              " tensor(16))"
            ]
          },
          "metadata": {},
          "execution_count": 438
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step- 8 Creating Data loader object\n"
      ],
      "metadata": {
        "id": "g1wHHhxrukfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True,pin_memory=True)"
      ],
      "metadata": {
        "id": "3uUc6mtAuoTn"
      },
      "execution_count": 439,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-9 Define the LSTM architecture"
      ],
      "metadata": {
        "id": "tqUBahoLuzFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMnetwork(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=100)\n",
        "    self.lstm = nn.LSTM(input_size=100,hidden_size=128,batch_first= True) #(batch_first = True)Explicitly telling put the batch dimension first\n",
        "    self.fc = nn.Linear(in_features=128,out_features=vocab_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    embedding = self.embedding(x)\n",
        "    intermediate_hidden_states,(final_hidden_state , final_cell_state) = self.lstm(embedding) # Each LSTM cell returns 3 outputs\n",
        "    output = self.fc(final_hidden_state.squeeze(0))\n",
        "    return output"
      ],
      "metadata": {
        "id": "XXS4ssd_vS_Q"
      },
      "execution_count": 440,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step -10 Define the training loop"
      ],
      "metadata": {
        "id": "CWT4xRag-y45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "p9KaRObZ-2dl"
      },
      "execution_count": 441,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMnetwork(len(vocab))\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "metadata": {
        "id": "6KE8Z5v1_GRt"
      },
      "execution_count": 442,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  total_epoch_loss = 0\n",
        "  for x,y in train_loader:\n",
        "    #1. Forward pass\n",
        "    y_pred = model(x)\n",
        "\n",
        "    #2. calculate loss\n",
        "    loss = loss_function(y_pred,y.squeeze(0))\n",
        "\n",
        "    #3. calculate gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    #4. Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    total_epoch_loss += loss.item()\n",
        "\n",
        "  avg_epoch_loss = total_epoch_loss/len(train_loader)\n",
        "  print(f\"Epoch: {epoch} Loss: {avg_epoch_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKZM4VVG_KdC",
        "outputId": "79c211a6-5387-4e21-a101-45ee247faa6b"
      },
      "execution_count": 443,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 5.117460791269938\n",
            "Epoch: 1 Loss: 3.382158406575521\n",
            "Epoch: 2 Loss: 1.9709965387980144\n",
            "Epoch: 3 Loss: 1.0977049748102823\n",
            "Epoch: 4 Loss: 0.6571684350570043\n",
            "Epoch: 5 Loss: 0.4759435618917147\n",
            "Epoch: 6 Loss: 0.3756946474313736\n",
            "Epoch: 7 Loss: 0.2650600877900918\n",
            "Epoch: 8 Loss: 0.24293631004790464\n",
            "Epoch: 9 Loss: 0.20850075831015905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tQPpbb0izHDI"
      },
      "execution_count": 443,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making prediction using LSTM"
      ],
      "metadata": {
        "id": "8KJ5TasaB_9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(model , sentence,vocab):\n",
        "\n",
        "  # STEP-1 Tokenize the input sentence\n",
        "  tokenized_sentence = word_tokenize(sentence.lower())\n",
        "\n",
        "  # Step-2 Convert the tokens into indices\n",
        "  numerical_sentence = tokens_to_indices(tokenized_sentence,vocab)\n",
        "\n",
        "  # Step-4 Padding the sequences\n",
        "  padded_sentence = torch.tensor(padding(numerical_sentence),dtype=torch.long)\n",
        "  padded_sentence = padded_sentence.unsqueeze(0)\n",
        "\n",
        "  # Step -5 Make prediction\n",
        "  output = model(padded_sentence)\n",
        "\n",
        "  # Step -6 Conver the logits into probabilities\n",
        "  output_prob = torch.softmax(output,dim=1)\n",
        "\n",
        "  # Step-7 Extract the indices of highest probability\n",
        "  _,index = torch.max(output_prob,dim=1)\n",
        "\n",
        "  # Step -4 return the merged text\n",
        "  return sentence+\" \"+list(vocab.keys())[index]\n"
      ],
      "metadata": {
        "id": "bhu4HIKwCDHX"
      },
      "execution_count": 444,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_tokens = 15\n",
        "input_text = \"You have to make\"\n",
        "\n",
        "for i in range (no_of_tokens):\n",
        "  output = make_prediction(model,input_text,vocab)\n",
        "  print(output)\n",
        "  input_text = output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ5GJ_3zCLCn",
        "outputId": "42aec0f2-b53e-4c23-f241-cbd77fabd312"
      },
      "execution_count": 450,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have to make all\n",
            "You have to make all your\n",
            "You have to make all your monthly\n",
            "You have to make all your monthly payments\n",
            "You have to make all your monthly payments on\n",
            "You have to make all your monthly payments on our\n",
            "You have to make all your monthly payments on our website\n",
            "You have to make all your monthly payments on our website .\n",
            "You have to make all your monthly payments on our website . here\n",
            "You have to make all your monthly payments on our website . here is\n",
            "You have to make all your monthly payments on our website . here is the\n",
            "You have to make all your monthly payments on our website . here is the link\n",
            "You have to make all your monthly payments on our website . here is the link for\n",
            "You have to make all your monthly payments on our website . here is the link for our\n",
            "You have to make all your monthly payments on our website . here is the link for our website\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the model accuracy"
      ],
      "metadata": {
        "id": "X4RCiq5xAR2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new data loader for test with same training data but shuffling\n",
        "test_loader = DataLoader(train_dataset,shuffle=True,batch_size=32)"
      ],
      "metadata": {
        "id": "Ym04KxcFAURh"
      },
      "execution_count": 446,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct =0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "\n",
        "  for x,y in test_loader:\n",
        "    # 1. Forward pass\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # 2. convert logits into probabilities\n",
        "\n",
        "    y_pred_probabilities = torch.softmax(y_pred,dim=1)\n",
        "\n",
        "    # 3. Extracting the index of highest probability token\n",
        "\n",
        "    _,output = torch.max(y_pred_probabilities,dim=1)\n",
        "\n",
        "    total += y.shape[0]\n",
        "    correct += (output ==y).sum().item()\n",
        "\n",
        "  accuracy = (correct / total) *100\n",
        "  print(f\"Accuracy: {accuracy}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGatEl1bAarS",
        "outputId": "b2b70d9a-0ed0-4558-c39f-adc70d3b4396"
      },
      "execution_count": 448,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 95.32908704883228%\n"
          ]
        }
      ]
    }
  ]
}